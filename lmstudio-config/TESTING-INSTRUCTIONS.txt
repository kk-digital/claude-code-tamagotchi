================================================================================
LM STUDIO TESTING INSTRUCTIONS
================================================================================
Date: 2025-11-05
Purpose: Comprehensive testing guide for LM Studio models on macOS
Covers: Embeddings, GPT-OSS models, Qwen3 Coder models

================================================================================
PREREQUISITES FOR TESTING
================================================================================

1. LM Studio installed and running
2. At least one model loaded (embeddings or LLM)
3. Local server started on port 1234
4. Terminal/command line access
5. curl and jq installed (jq for JSON parsing)

**Install jq if needed:**
```bash
brew install jq
```

================================================================================
TEST SUITE 1: BASIC CONNECTIVITY
================================================================================

**Test 1.1: Check Server is Running**

```bash
curl -s http://localhost:1234/v1/models | jq '.'
```

Expected: JSON list of loaded models
Success: Returns model list without errors
Failure: "Connection refused" → Server not running

**Test 1.2: List Loaded Models**

```bash
curl -s http://localhost:1234/v1/models | jq '.data[].id'
```

Expected: Model IDs (e.g., "openai/gpt-oss-20b", "text-embedding-qwen3-embedding-8b")
Success: Lists all loaded models
Failure: Empty list → No models loaded

**Test 1.3: Check Model Details**

```bash
curl -s http://localhost:1234/v1/models | jq '.data[] | {id, object, created}'
```

Expected: Model metadata with timestamps

================================================================================
TEST SUITE 2: EMBEDDING MODELS
================================================================================

**Test 2.1: Single Text Embedding**

```bash
curl -X POST http://localhost:1234/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "text-embedding-qwen3-embedding-8b",
    "input": "Hello world"
  }' | jq '.data[0].embedding | length'
```

Expected: 4096 (for qwen3-8b), 768 (for nomic-embed/granite-125m)
Success: Returns correct dimension count
Failure: "Model is not embedding" → Wrong model type

**Test 2.2: Batch Embedding**

```bash
curl -X POST http://localhost:1234/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "text-embedding-qwen3-embedding-8b",
    "input": ["First text", "Second text", "Third text"]
  }' | jq '.data | length'
```

Expected: 3
Success: Returns 3 embeddings
Failure: Error → Model doesn't support batch

**Test 2.3: Empty Input**

```bash
curl -X POST http://localhost:1234/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "text-embedding-qwen3-embedding-8b",
    "input": ""
  }' | jq '.error'
```

Expected: Error message or null embedding
Success: Handles gracefully

**Test 2.4: Long Text Embedding**

```bash
# Generate long text (1000 words)
LONG_TEXT=$(python3 -c "print(' '.join(['word'] * 1000))")

curl -X POST http://localhost:1234/v1/embeddings \
  -H "Content-Type: application/json" \
  -d "{
    \"model\": \"text-embedding-qwen3-embedding-8b\",
    \"input\": \"$LONG_TEXT\"
  }" | jq '.data[0].embedding | length'
```

Expected: 4096 (handles long text)
Success: Returns embedding

**Test 2.5: Performance Test (100 embeddings)**

```bash
time for i in {1..100}; do
  curl -s -X POST http://localhost:1234/v1/embeddings \
    -H "Content-Type: application/json" \
    -d '{
      "model": "text-embedding-qwen3-embedding-8b",
      "input": "test text '$i'"
    }' > /dev/null
done
```

Expected: Completes in reasonable time
Success: <60 seconds for 100 embeddings
Note: Time varies by model and hardware

**Test 2.6: All Embedding Models**

```bash
for model in "text-embedding-nomic-embed-text-v1.5" "text-embedding-granite-embedding-125m-english" "text-embedding-qwen3-embedding-0.6b" "text-embedding-qwen3-embedding-4b" "text-embedding-qwen3-embedding-8b" "text-embedding-embeddinggemma-300m-with-dense-modules"; do
  echo "Testing: $model"
  curl -s -X POST http://localhost:1234/v1/embeddings \
    -H "Content-Type: application/json" \
    -d "{\"model\": \"$model\", \"input\": \"test\"}" | \
    jq -r 'if .error then "❌ ERROR: \(.error.message)" else "✅ SUCCESS - Dimensions: \(.data[0].embedding | length)" end'
  echo ""
done
```

Expected: Each model returns its dimension count
Success: All working models return embeddings

================================================================================
TEST SUITE 3: GPT-OSS CHAT MODELS
================================================================================

**Test 3.1: Simple Chat Completion**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-oss-20b",
    "messages": [{"role": "user", "content": "What is 2+2?"}],
    "temperature": 0.7,
    "max_tokens": 100
  }' | jq '.choices[0].message.content'
```

Expected: "4" or explanation
Success: Returns correct answer
Failure: Error or timeout

**Test 3.2: Chat with System Prompt**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-oss-20b",
    "messages": [
      {"role": "system", "content": "You are a helpful coding assistant."},
      {"role": "user", "content": "Write a Python hello world"}
    ],
    "temperature": 0.7,
    "max_tokens": 300
  }' | jq '.choices[0].message.content'
```

Expected: Python code snippet
Success: Returns valid Python code

**Test 3.3: Multi-turn Conversation**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-oss-20b",
    "messages": [
      {"role": "user", "content": "My name is Alice"},
      {"role": "assistant", "content": "Nice to meet you, Alice!"},
      {"role": "user", "content": "What is my name?"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }' | jq '.choices[0].message.content'
```

Expected: "Alice" or "Your name is Alice"
Success: Remembers context

**Test 3.4: Streaming Response**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-oss-20b",
    "messages": [{"role": "user", "content": "Count from 1 to 10"}],
    "stream": true
  }'
```

Expected: Streaming SSE events
Success: Receives tokens progressively

**Test 3.5: Temperature Variations**

```bash
for temp in 0.1 0.5 1.0; do
  echo "Temperature: $temp"
  curl -s -X POST http://localhost:1234/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d "{
      \"model\": \"openai/gpt-oss-20b\",
      \"messages\": [{\"role\": \"user\", \"content\": \"Write a creative story opening\"}],
      \"temperature\": $temp,
      \"max_tokens\": 100
    }" | jq -r '.choices[0].message.content'
  echo "---"
done
```

Expected: Lower temp = more deterministic, higher = more creative
Success: Different outputs for different temperatures

**Test 3.6: Token Usage Tracking**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-oss-20b",
    "messages": [{"role": "user", "content": "Explain AI in one paragraph"}],
    "max_tokens": 500
  }' | jq '.usage'
```

Expected: {prompt_tokens, completion_tokens, total_tokens}
Success: Returns token counts

**Test 3.7: Max Tokens Limit**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-oss-20b",
    "messages": [{"role": "user", "content": "Write a long story"}],
    "max_tokens": 50
  }' | jq '.choices[0].message.content | length'
```

Expected: Response truncated at ~50 tokens
Success: Respects max_tokens limit

**Test 3.8: JSON Mode (if supported)**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-oss-20b",
    "messages": [
      {"role": "system", "content": "Respond in JSON format"},
      {"role": "user", "content": "Give me info about cats"}
    ],
    "temperature": 0.7,
    "max_tokens": 300
  }' | jq '.choices[0].message.content | fromjson?'
```

Expected: Valid JSON response
Success: Parses as JSON

================================================================================
TEST SUITE 4: QWEN3 CODER MODELS
================================================================================

**Test 4.1: Simple Code Generation**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-coder-7b",
    "messages": [
      {"role": "system", "content": "You are a code generation assistant."},
      {"role": "user", "content": "Write a Python function to reverse a string"}
    ],
    "temperature": 0.3,
    "max_tokens": 500
  }' | jq -r '.choices[0].message.content'
```

Expected: Python function with code
Success: Returns valid Python code

**Test 4.2: Code Review**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-coder-7b",
    "messages": [
      {"role": "system", "content": "You are a code reviewer."},
      {"role": "user", "content": "Review this code:\n\ndef add(x, y):\n  return x + y\n\nresult = add(\"5\", \"3\")"}
    ],
    "temperature": 0.4,
    "max_tokens": 1000
  }' | jq -r '.choices[0].message.content'
```

Expected: Identifies type issue (string concatenation)
Success: Catches the bug

**Test 4.3: Multi-Language Support**

```bash
for lang in Python JavaScript TypeScript Go Rust; do
  echo "Testing $lang:"
  curl -s -X POST http://localhost:1234/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d "{
      \"model\": \"qwen3-coder-7b\",
      \"messages\": [{\"role\": \"user\", \"content\": \"Write a hello world program in $lang\"}],
      \"temperature\": 0.3,
      \"max_tokens\": 300
    }" | jq -r '.choices[0].message.content'
  echo "---"
done
```

Expected: Code in each language
Success: Generates valid code for all languages

**Test 4.4: Code Completion**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-coder-0.5b",
    "messages": [
      {"role": "user", "content": "Complete this function:\n\ndef fibonacci(n):\n    # Complete"}
    ],
    "temperature": 0.2,
    "max_tokens": 300
  }' | jq -r '.choices[0].message.content'
```

Expected: Fibonacci implementation
Success: Returns working code

**Test 4.5: Debugging Assistance**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-coder-7b",
    "messages": [
      {"role": "system", "content": "You are a debugging expert."},
      {"role": "user", "content": "Why does this fail?\n\nx = [1, 2, 3]\nprint(x[3])"}
    ],
    "temperature": 0.3,
    "max_tokens": 500
  }' | jq -r '.choices[0].message.content'
```

Expected: Explains IndexError
Success: Identifies the bug

================================================================================
TEST SUITE 5: PERFORMANCE TESTS
================================================================================

**Test 5.1: Embedding Generation Speed**

```bash
echo "Testing embedding speed..."
time curl -s -X POST http://localhost:1234/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "text-embedding-qwen3-embedding-8b",
    "input": "Performance test"
  }' > /dev/null
```

Expected: <2 seconds
Success: Fast response

**Test 5.2: Chat Tokens Per Second**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-oss-20b",
    "messages": [{"role": "user", "content": "Write a 200-word essay about AI"}],
    "max_tokens": 300
  }' | jq '.usage'
```

Expected: Shows tokens generated
Success: Calculate tokens/second manually

**Test 5.3: Concurrent Requests (Load Test)**

```bash
# Test 10 concurrent requests
for i in {1..10}; do
  (curl -s -X POST http://localhost:1234/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d "{
      \"model\": \"openai/gpt-oss-20b\",
      \"messages\": [{\"role\": \"user\", \"content\": \"Test $i\"}],
      \"max_tokens\": 50
    }" > /dev/null) &
done
wait
echo "All requests completed"
```

Expected: All complete without errors
Success: Handles concurrent load

================================================================================
TEST SUITE 6: ERROR HANDLING
================================================================================

**Test 6.1: Invalid Model Name**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nonexistent-model",
    "messages": [{"role": "user", "content": "test"}]
  }' | jq '.error'
```

Expected: Error message
Success: Returns meaningful error

**Test 6.2: Missing Required Field**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-oss-20b"
  }' | jq '.error'
```

Expected: "messages field required" error
Success: Validation works

**Test 6.3: Empty Messages Array**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-oss-20b",
    "messages": []
  }' | jq '.error'
```

Expected: Error about empty messages
Success: Catches invalid input

**Test 6.4: Invalid Temperature**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-oss-20b",
    "messages": [{"role": "user", "content": "test"}],
    "temperature": 5.0
  }' | jq '.error'
```

Expected: Error or clamps to valid range
Success: Handles gracefully

================================================================================
TEST SUITE 7: INTEGRATION TESTS
================================================================================

**Test 7.1: Claude Code Tamagotchi Integration**

```bash
# Assuming project is set up with LM Studio
cd /path/to/claude-code-tamagotchi
./test-complete.sh
```

Expected: Tests pass using LM Studio backend
Success: Application works with local LLM

**Test 7.2: Environment Variable Test**

```bash
# Create test .env
echo "LM_STUDIO_ENABLED=true" > test.env
echo "LM_STUDIO_URL=http://localhost:1234/v1" >> test.env
echo "LM_STUDIO_MODEL=openai/gpt-oss-20b" >> test.env

# Test loading
source test.env
curl -s -X POST $LM_STUDIO_URL/chat/completions \
  -H "Content-Type: application/json" \
  -d "{
    \"model\": \"$LM_STUDIO_MODEL\",
    \"messages\": [{\"role\": \"user\", \"content\": \"test\"}]
  }" | jq '.choices[0].message.content'

rm test.env
```

Expected: Works with env variables
Success: Configuration loaded correctly

================================================================================
AUTOMATED TEST SCRIPT
================================================================================

**Save as test-lmstudio.sh:**

```bash
#!/bin/bash
# LM Studio Comprehensive Test Script

echo "=== LM STUDIO TEST SUITE ==="
echo ""

# Test 1: Server connectivity
echo "Test 1: Server connectivity..."
if curl -s http://localhost:1234/v1/models > /dev/null; then
  echo "✅ PASS: Server is running"
else
  echo "❌ FAIL: Server not running"
  exit 1
fi

# Test 2: Embedding model
echo "Test 2: Embedding model..."
DIMS=$(curl -s -X POST http://localhost:1234/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{"model": "text-embedding-qwen3-embedding-8b", "input": "test"}' | \
  jq '.data[0].embedding | length')
if [ "$DIMS" = "4096" ]; then
  echo "✅ PASS: Embedding works (4096 dims)"
else
  echo "⚠️  WARN: Embedding returned $DIMS dims"
fi

# Test 3: Chat model
echo "Test 3: Chat completion..."
RESPONSE=$(curl -s -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "openai/gpt-oss-20b", "messages": [{"role": "user", "content": "Say hello"}], "max_tokens": 50}' | \
  jq -r '.choices[0].message.content')
if [ -n "$RESPONSE" ]; then
  echo "✅ PASS: Chat completion works"
  echo "   Response: ${RESPONSE:0:50}..."
else
  echo "❌ FAIL: No response from chat"
fi

# Test 4: Token usage
echo "Test 4: Token usage tracking..."
TOKENS=$(curl -s -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "openai/gpt-oss-20b", "messages": [{"role": "user", "content": "test"}], "max_tokens": 10}' | \
  jq '.usage.total_tokens')
if [ -n "$TOKENS" ]; then
  echo "✅ PASS: Token tracking works ($TOKENS tokens)"
else
  echo "⚠️  WARN: Token usage not tracked"
fi

echo ""
echo "=== TEST SUITE COMPLETE ==="
```

**Run test script:**
```bash
chmod +x test-lmstudio.sh
./test-lmstudio.sh
```

================================================================================
TROUBLESHOOTING FAILED TESTS
================================================================================

**"Connection refused":**
- LM Studio server not running
- Check "Local Server" tab in LM Studio
- Verify port 1234 is not blocked

**"Model is not embedding":**
- Wrong model type loaded
- Load embedding model instead
- Check model name in curl command

**Timeout errors:**
- Model taking too long
- Increase timeout in curl: --max-time 60
- Use faster/smaller model

**Invalid JSON errors:**
- Check JSON syntax in curl command
- Escape quotes properly
- Use jq to validate JSON

**Empty responses:**
- Model not properly loaded
- Check LM Studio UI shows "Loaded"
- Reload model if needed

================================================================================
END OF TESTING INSTRUCTIONS
================================================================================
