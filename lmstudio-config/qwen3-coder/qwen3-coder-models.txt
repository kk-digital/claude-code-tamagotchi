================================================================================
QWEN3 CODER MODELS - CONFIGURATION AND PRESETS
================================================================================
Date: 2025-11-05
Purpose: Configuration for Qwen3 Coder models in LM Studio on macOS
Models: Specialized coding/programming models from Qwen3 family

================================================================================
QWEN3 CODER MODEL FAMILY
================================================================================

✅ **qwen3-coder-0.5b** (ULTRAFAST)
   - Parameters: 0.5 billion
   - Size: ~300 MB (Q4_K_M)
   - Context: 32,768 tokens
   - Best for: Code completion, inline suggestions, fast iteration
   - Speed: 100-200 tokens/sec
   - RAM: 2 GB minimum
   - Use case: Real-time coding assistant, low-latency completions

✅ **qwen3-coder-1.5b** (FAST)
   - Parameters: 1.5 billion
   - Size: ~900 MB (Q4_K_M)
   - Context: 32,768 tokens
   - Best for: Code generation, simple refactoring, documentation
   - Speed: 60-100 tokens/sec
   - RAM: 4 GB minimum
   - Use case: Fast code generation, quick explanations

✅ **qwen3-coder-7b** (BALANCED)
   - Parameters: 7 billion
   - Size: ~4 GB (Q4_K_M)
   - Context: 32,768 tokens
   - Best for: Complex code generation, debugging, architecture
   - Speed: 30-50 tokens/sec
   - RAM: 8 GB minimum
   - Use case: General-purpose coding assistant
   - RECOMMENDED FOR MOST USERS

✅ **qwen3-coder-14b** (HIGH QUALITY)
   - Parameters: 14 billion
   - Size: ~8 GB (Q4_K_M)
   - Context: 32,768 tokens
   - Best for: Advanced refactoring, algorithm design, code review
   - Speed: 20-35 tokens/sec
   - RAM: 16 GB minimum
   - Use case: Professional development, complex tasks

✅ **qwen3-coder-32b** (EXPERT)
   - Parameters: 32 billion
   - Size: ~18 GB (Q4_K_M)
   - Context: 32,768 tokens
   - Best for: System design, performance optimization, security analysis
   - Speed: 12-20 tokens/sec
   - RAM: 24 GB minimum
   - Use case: Expert-level coding assistance, architectural decisions
   - RECOMMENDED FOR PRODUCTION

================================================================================
PRESET CONFIGURATIONS
================================================================================

**PRESET 1: QWEN3-CODER-0.5B ULTRAFAST** (Code Completion)
Model: qwen3-coder-0.5b
Quantization: Q4_K_M
GPU Layers: 20 (full offload)
Context Length: 8192 (for speed)
Temperature: 0.2 (deterministic)
Top P: 0.9
Max Tokens: 512 (short completions)
Repeat Penalty: 1.05
Use case: Inline code completion, autocomplete, fast suggestions

**PRESET 2: QWEN3-CODER-7B BALANCED** (General Coding)
Model: qwen3-coder-7b
Quantization: Q4_K_M
GPU Layers: 35 (full offload)
Context Length: 16384
Temperature: 0.3
Top P: 0.85
Max Tokens: 2048
Repeat Penalty: 1.1
Use case: General code generation, debugging, documentation
RECOMMENDED FOR MOST USERS

**PRESET 3: QWEN3-CODER-32B EXPERT** (Complex Tasks)
Model: qwen3-coder-32b
Quantization: Q4_K_M
GPU Layers: 45 (full offload if VRAM available)
Context Length: 32768 (full context)
Temperature: 0.4
Top P: 0.8
Max Tokens: 4096
Repeat Penalty: 1.15
Use case: System architecture, performance optimization, code review

**PRESET 4: QWEN3-CODER-14B DEVELOPMENT** (Daily Development)
Model: qwen3-coder-14b
Quantization: Q4_K_M
GPU Layers: 40
Context Length: 16384
Temperature: 0.3
Top P: 0.85
Max Tokens: 3072
Repeat Penalty: 1.1
Use case: Professional development workflow, refactoring, testing

================================================================================
MODEL SELECTION GUIDE
================================================================================

**Choose based on task:**

| Task Type                 | Recommended Model  | Speed    | Quality  |
|---------------------------|--------------------|----------|----------|
| Code completion           | qwen3-coder-0.5b   | Fastest  | Good     |
| Simple generation         | qwen3-coder-1.5b   | Fast     | Good     |
| General coding            | qwen3-coder-7b     | Balanced | Very Good|
| Complex refactoring       | qwen3-coder-14b    | Moderate | Excellent|
| Architecture/optimization | qwen3-coder-32b    | Slower   | Expert   |

**Programming Language Support:**
- Primary: Python, JavaScript/TypeScript, Java, C/C++, Go, Rust
- Good: PHP, Ruby, Swift, Kotlin, C#, SQL
- Supported: Most popular languages including HTML/CSS

**Context Window Usage:**
- Code completion: 2K-4K tokens (just local context)
- Function generation: 8K-16K tokens (file context)
- Refactoring: 16K-32K tokens (multi-file context)

================================================================================
SYSTEM PROMPTS FOR CODING TASKS
================================================================================

**SYSTEM PROMPT 1: CODE GENERATION**

You are an expert programming assistant. Generate clean, efficient, well-documented code.
Follow these principles:
- Write idiomatic code for the target language
- Include type hints/annotations when applicable
- Add brief comments for complex logic
- Follow standard naming conventions
- Consider edge cases and error handling

**SYSTEM PROMPT 2: CODE REVIEW**

You are a senior code reviewer. Analyze code for:
- Correctness and potential bugs
- Performance and efficiency
- Security vulnerabilities
- Code style and best practices
- Documentation completeness
Provide specific, actionable feedback.

**SYSTEM PROMPT 3: DEBUGGING ASSISTANT**

You are a debugging expert. Help identify and fix issues by:
- Analyzing error messages and stack traces
- Suggesting debugging strategies
- Identifying root causes
- Providing step-by-step fix instructions
- Explaining why the bug occurred

**SYSTEM PROMPT 4: REFACTORING ASSISTANT**

You are a refactoring expert. Improve code by:
- Identifying code smells
- Suggesting design pattern improvements
- Improving readability and maintainability
- Reducing complexity and duplication
- Preserving functionality while improving structure

================================================================================
ENVIRONMENT VARIABLES
================================================================================

Add to your .env file for coding assistant integration:

# Qwen3 Coder Configuration
LM_STUDIO_CODER_MODEL=qwen3-coder-7b  # or 14b, 32b
LM_STUDIO_CODER_URL=http://localhost:1234/v1
LM_STUDIO_CODER_TEMPERATURE=0.3  # Lower for more deterministic code
LM_STUDIO_CODER_MAX_TOKENS=2048

# Code Completion Settings
CODE_COMPLETION_ENABLED=true
CODE_COMPLETION_TEMPERATURE=0.2  # Very deterministic
CODE_COMPLETION_MAX_TOKENS=512  # Short completions

# Code Review Settings
CODE_REVIEW_MODEL=qwen3-coder-32b  # Use largest for reviews
CODE_REVIEW_TEMPERATURE=0.4

================================================================================
TESTING QWEN3 CODER MODELS
================================================================================

**Test 1: Simple Code Generation**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-coder-7b",
    "messages": [
      {"role": "system", "content": "You are an expert programming assistant."},
      {"role": "user", "content": "Write a Python function to reverse a string"}
    ],
    "temperature": 0.3,
    "max_tokens": 500
  }' | jq '.choices[0].message.content'
```

**Test 2: Code Review**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-coder-14b",
    "messages": [
      {"role": "system", "content": "You are a code reviewer."},
      {"role": "user", "content": "Review this Python code:\n\ndef add(x, y):\n  return x + y\n\nresult = add(\"5\", \"3\")"}
    ],
    "temperature": 0.4,
    "max_tokens": 1000
  }' | jq '.choices[0].message.content'
```

**Test 3: Debugging Help**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-coder-7b",
    "messages": [
      {"role": "system", "content": "You are a debugging expert."},
      {"role": "user", "content": "I get TypeError: can only concatenate str (not \"int\") to str. How do I fix it?"}
    ],
    "temperature": 0.3,
    "max_tokens": 800
  }' | jq '.choices[0].message.content'
```

**Test 4: Code Completion**

```bash
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-coder-0.5b",
    "messages": [
      {"role": "user", "content": "Complete this function:\n\ndef fibonacci(n):\n    # Complete this"}
    ],
    "temperature": 0.2,
    "max_tokens": 300
  }' | jq '.choices[0].message.content'
```

**Test 5: Multi-Language Support**

```bash
# Test Python
curl -s -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen3-coder-7b", "messages": [{"role": "user", "content": "Write a Python decorator for timing functions"}], "temperature": 0.3}' | jq -r '.choices[0].message.content'

echo "---"

# Test JavaScript
curl -s -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen3-coder-7b", "messages": [{"role": "user", "content": "Write a JavaScript async/await example"}], "temperature": 0.3}' | jq -r '.choices[0].message.content'
```

================================================================================
PERFORMANCE BENCHMARKS (M1 Max, 32 GB RAM)
================================================================================

**Code Generation Speed:**
- qwen3-coder-0.5b: 150-200 tokens/sec (GPU)
- qwen3-coder-1.5b: 80-120 tokens/sec (GPU)
- qwen3-coder-7b: 40-60 tokens/sec (GPU)
- qwen3-coder-14b: 25-40 tokens/sec (GPU)
- qwen3-coder-32b: 15-25 tokens/sec (GPU)

**Load Time:**
- qwen3-coder-0.5b: ~5 seconds
- qwen3-coder-1.5b: ~10 seconds
- qwen3-coder-7b: ~20 seconds
- qwen3-coder-14b: ~35 seconds
- qwen3-coder-32b: ~60 seconds

**Memory Usage:**
- qwen3-coder-0.5b: ~500 MB RAM, ~400 MB VRAM
- qwen3-coder-1.5b: ~1.2 GB RAM, ~1 GB VRAM
- qwen3-coder-7b: ~5 GB RAM, ~4 GB VRAM
- qwen3-coder-14b: ~10 GB RAM, ~8 GB VRAM
- qwen3-coder-32b: ~20 GB RAM, ~16 GB VRAM

================================================================================
CODE QUALITY COMPARISON
================================================================================

**Task: Implement Binary Search**

qwen3-coder-0.5b:
- Basic implementation, works correctly
- Minimal comments
- Standard variable names
- Quality: 7/10

qwen3-coder-7b:
- Well-structured implementation
- Good comments and edge case handling
- Type hints included
- Quality: 8.5/10

qwen3-coder-32b:
- Expert-level implementation
- Comprehensive documentation
- Multiple approaches discussed
- Performance optimization notes
- Quality: 9.5/10

================================================================================
RECOMMENDED WORKFLOWS
================================================================================

**Workflow 1: Fast Iteration (Development)**
- Model: qwen3-coder-7b
- Temperature: 0.3
- Use for: Daily coding, prototyping, quick fixes
- Speed: ~40-60 tokens/sec

**Workflow 2: Code Review (Quality)**
- Model: qwen3-coder-32b
- Temperature: 0.4
- Use for: Pull request reviews, architecture decisions
- Speed: ~15-25 tokens/sec

**Workflow 3: Code Completion (Real-time)**
- Model: qwen3-coder-0.5b
- Temperature: 0.2
- Use for: IDE integration, autocomplete
- Speed: ~150-200 tokens/sec

**Workflow 4: Learning/Explanation**
- Model: qwen3-coder-14b
- Temperature: 0.5
- Use for: Code explanations, teaching, documentation
- Speed: ~25-40 tokens/sec

================================================================================
INTEGRATION EXAMPLES
================================================================================

**VSCode Extension Integration:**

```javascript
// Example VSCode extension code
const response = await fetch('http://localhost:1234/v1/chat/completions', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    model: 'qwen3-coder-7b',
    messages: [
      { role: 'system', content: 'You are a code completion assistant.' },
      { role: 'user', content: `Complete this code:\n${currentCode}` }
    ],
    temperature: 0.2,
    max_tokens: 300
  })
});
```

**CLI Coding Assistant:**

```bash
#!/bin/bash
# coding-assistant.sh - Simple CLI coding helper

MODEL="qwen3-coder-7b"
QUESTION="$1"

curl -s -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d "{
    \"model\": \"$MODEL\",
    \"messages\": [{\"role\": \"user\", \"content\": \"$QUESTION\"}],
    \"temperature\": 0.3,
    \"max_tokens\": 2000
  }" | jq -r '.choices[0].message.content'
```

Usage:
```bash
./coding-assistant.sh "Write a Python function to parse JSON"
```

================================================================================
TROUBLESHOOTING
================================================================================

**Model generates invalid code:**
- Lower temperature (0.2-0.3 for more deterministic)
- Add more specific system prompt
- Increase context with examples

**Code completions too slow:**
- Use smaller model (qwen3-coder-0.5b or 1.5b)
- Reduce max_tokens to 200-500
- Increase GPU layers

**Model doesn't understand language:**
- Specify language explicitly in prompt
- Provide language-specific examples
- Use qwen3-coder-7b or larger

**Inconsistent code style:**
- Add style guide to system prompt
- Lower temperature (0.2-0.3)
- Provide style examples in context

================================================================================
COMPARING TO OTHER CODE MODELS
================================================================================

**vs GitHub Copilot:**
- Copilot: Cloud-based, faster, integrated
- Qwen3 Coder: Local, private, customizable

**vs CodeLlama:**
- CodeLlama-7b: Similar quality to qwen3-coder-7b
- Qwen3 Coder: Better at non-English, longer context

**vs GPT-4 (coding):**
- GPT-4: Better at complex architecture
- Qwen3 Coder 32b: Close quality, local/private

**vs StarCoder:**
- StarCoder: Trained on more languages
- Qwen3 Coder: Better instruction following

================================================================================
BEST PRACTICES
================================================================================

1. **Use appropriate model size:**
   - Small tasks: 0.5b-1.5b
   - Daily coding: 7b-14b
   - Complex tasks: 32b

2. **Set low temperature for code:**
   - Code generation: 0.2-0.3
   - Code review: 0.3-0.4
   - Creative coding: 0.5-0.6

3. **Provide context:**
   - Include surrounding code
   - Specify language and version
   - Mention framework/library if relevant

4. **Use system prompts:**
   - Define coding standards
   - Specify output format
   - Set expertise level

5. **Batch similar requests:**
   - Group completions together
   - Use streaming for real-time
   - Cache common patterns

================================================================================
END OF QWEN3 CODER CONFIGURATION
================================================================================
